{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "# (Author: Jan Klinkosz, id number: 394 342, kaggle nick: Johny7013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to import python modules that are necessary to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('cooking_train.json')\n",
    "test = pd.read_json('cooking_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global data & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(123)\n",
    "models = {}\n",
    "predictions_from_models = {}\n",
    "\n",
    "number_of_cuisines = len(set(train.cuisine))\n",
    "\n",
    "def initialise_dict(size):\n",
    "    d = {}\n",
    "    for i in range(size):\n",
    "        d[i] = 0\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genarally, this solution is founded of extra trees models, wich were build for every pair of cuisines. After building N * (N - 1) models we hold a turnament (or series of turnament) where we every cuisine \"play\" with every other cuisine, wich means that we check what value was returned by model dedicated to decide between cuisine1 and cuisine2. We count number of models, wich pick particular cuisine and k cuisines with the best scores go to another tournament (another tournament is hold only with this k cuisines). At the end our prediction for particular record is the cuisine that stayed after all tournaments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation behind this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found out that extra trees are quite good in deciding between just 2 cuisines (It is much simpler task, and cross validation on this \"pair models\" was really promising, because I got quite good accuracy ~ 85 - 90 on most of them, sometimes 75 - 78. I thought that if it simpler to get answear between just two of cuisines then the right one should get high score on its N - 1 models, and rest of them sth around expected value for random sampling (of course it's not precisie, because some of the cuisines are really similar to another but it's just my intuition). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not all of them used in preprocessor beacause some of them gave worse results than the others\n",
    "\n",
    "def has_no_numbers(input_string):\n",
    "    return not bool(re.search(r'[\\d]', input_string))\n",
    "\n",
    "\n",
    "def is_noun(input_string):\n",
    "    tmp = wn.synsets(input_string)\n",
    "\n",
    "    # don't add if it isn't in dictionary\n",
    "    \n",
    "    return tmp and tmp[0].pos() == \"n\"\n",
    "\n",
    "\n",
    "def is_noun_plus_unrecognised(input_string):\n",
    "    tmp = wn.synsets(input_string)\n",
    "\n",
    "    # if it is not in the dictionary add it too (just in case)\n",
    "    # or if it is a noun (obviously add)\n",
    "\n",
    "    return not tmp or tmp[0].pos() == \"n\"\n",
    "\n",
    "\n",
    "def is_actual_ingredient(input_string):\n",
    "    return has_no_numbers(input_string) and is_noun(input_string)\n",
    "\n",
    "\n",
    "# line of words to single ones\n",
    "def to_single_words(line):\n",
    "    return ' '.join(line).lower().split()\n",
    "\n",
    "\n",
    "# remove letter s from the end of words\n",
    "# create singular from plural (heuristic approach)\n",
    "def to_singular_form(s):\n",
    "    if s[len(s) - 1] == 's':\n",
    "        x = s[0:len(s) - 1]\n",
    "    else:\n",
    "        x = s\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocessor(line):\n",
    "    single_words = to_single_words(line)\n",
    "    without_numbers = list(filter(has_no_numbers, single_words))\n",
    "    regularised = list(map(to_singular_form, without_numbers))\n",
    "    return ' '.join(regularised).lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to hold tournament\n",
    "# list_of_tournament_cuisines - cuisines in tournament\n",
    "# number_of_winners - best number_of_winners cuisine's guesses to pick\n",
    "def make_tournament(list_of_tournament_cuisines, number_of_winners, number_of_prediction):\n",
    "    d = initialise_dict(number_of_cuisines)\n",
    "\n",
    "    for first in range(len(list_of_tournament_cuisines)):\n",
    "        for second in range(first + 1, len(list_of_tournament_cuisines)):\n",
    "            d[predictions_from_models[(list_of_tournament_cuisines[first], list_of_tournament_cuisines[second])][number_of_prediction]] += 1\n",
    "\n",
    "    pairs_from_d = [(d[k], k) for k in d]\n",
    "    pairs_from_d.sort(reverse=True)\n",
    "\n",
    "    winners = []\n",
    "\n",
    "    for k in range(number_of_winners):\n",
    "        winners.append(pairs_from_d[k][1])\n",
    "\n",
    "    winners.sort()\n",
    "\n",
    "    return winners\n",
    "\n",
    "# predict cuisines for test X on all N * (N - 1) models\n",
    "def predict(X):\n",
    "\n",
    "    for m in range(number_of_cuisines):\n",
    "        for n in range(m + 1, number_of_cuisines):\n",
    "            predictions_from_models[(m, n)] = models[(m, n)].predict(X)\n",
    "\n",
    "    result = np.zeros(len(X))\n",
    "\n",
    "    for m in range(len(X)):\n",
    "        # get 5 cuisines with the best score\n",
    "        winners_tournament1 = make_tournament(range(number_of_cuisines), 5, m)\n",
    "        \n",
    "        # get 2 cuisines with the best score from 5 that remained\n",
    "        winners_tournament2 = make_tournament(winners_tournament1, 2, m)\n",
    "\n",
    "        # get winner from last 2\n",
    "        winner = make_tournament(winners_tournament2, 1, m)\n",
    "\n",
    "        result[m] = winner[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for training and genarating a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = train.ingredients\n",
    "\n",
    "# use preprocessor\n",
    "vect = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "# make one-hot vectors from recipes\n",
    "vectors = vect.fit_transform(recipes).todense()\n",
    "\n",
    "ingredients_one_hot_vectors = pd.DataFrame(data=vectors, columns=sorted(vect.vocabulary_))\n",
    "\n",
    "X_train = ingredients_one_hot_vectors\n",
    "y_train = train['cuisine']\n",
    "\n",
    "# cuisines to integers\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder = encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "# concatenate training data with cuisine types\n",
    "data = pd.concat([X_train, pd.Series(y_train).rename(\"cuisine_type\")], axis=1)\n",
    "data.reset_index()\n",
    "\n",
    "cuisines_data = {}\n",
    "\n",
    "# split data from particular cuisine into one bucket\n",
    "for i in range(number_of_cuisines):\n",
    "    cuisine_i = data.loc[data[\"cuisine_type\"] == i]\n",
    "    cuisines_data[i] = (cuisine_i.drop(columns=\"cuisine_type\"), cuisine_i[\"cuisine_type\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't used it when generating final score\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=323)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params for generating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I submitted with n_estimators equals to 600 but it requires sth around 12 GB RAM so in notebook I decided\n",
    "# to cut this parameter a little\n",
    "# it may take some time to build all models especially with 600\n",
    "et_pipeline_pair_models = Pipeline([\n",
    "    ('classifier', ExtraTreesClassifier(n_estimators=50, random_state=671232, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building models (could take some time - around 1 minute on my computer for n_estimators=50)\n",
    "\n",
    "for i in range(number_of_cuisines):\n",
    "    for j in range(i + 1, number_of_cuisines):\n",
    "        X = pd.concat([cuisines_data[i][0], cuisines_data[j][0]], axis=0)\n",
    "        y = pd.concat([cuisines_data[i][1], cuisines_data[j][1]], axis=0)\n",
    "        models[(i, j)] = et_pipeline_pair_models.fit(X, y)\n",
    "        et_pipeline_pair_models = clone(et_pipeline_pair_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for testing with split (split also commented above)\n",
    "# predicting\n",
    "#prediction = predict(X_test)\n",
    "\n",
    "# check accuracy\n",
    "#print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genrate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['ingredients']\n",
    "\n",
    "# transform test data to appropriate form\n",
    "vectors = vect.transform(X_test).todense()\n",
    "X_test = pd.DataFrame(data=vectors, columns=sorted(vect.vocabulary_))\n",
    "\n",
    "# could take some time too\n",
    "prediction = predict(X_test)\n",
    "\n",
    "# go back from integer to cuisine names\n",
    "prediction = encoder.inverse_transform(prediction.astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate csv file with predictions to records with y value equal to unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory where predictions shall be placed\n",
    "result_name = \"jk394342_predictions_solution1.csv\"\n",
    "\n",
    "submission = test.copy()\n",
    "submission['cuisine'] = prediction\n",
    "submission.to_csv(result_name, index=False, columns=['id', 'cuisine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution number 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between solution number 1 and solution number 2 is that in solution number two there are no turnament. Predict function works this way: we randomly pick 2 cuisines form all of the cuisines, we get dedicated model for this pair of cuisines and we check which cousine is picked. The one picked stays in the set of cuisines and the other one is thrown away. We repeat this until there is only one cuisine left. This is our prediction for this record. Intuition behind this solution is quite similar to the one behind solution number 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "\n",
    "    for m in range(number_of_cuisines):\n",
    "        for n in range(m + 1, number_of_cuisines):\n",
    "            predictions_from_models[(m, n)] = models[(m, n)].predict(X)\n",
    "\n",
    "    result = np.zeros(len(X))\n",
    "\n",
    "    for m in range(len(X)):\n",
    "        x = number_of_cuisines - 1\n",
    "        cuisines_numbers = list(range(number_of_cuisines))\n",
    "        \n",
    "        for n in range(number_of_cuisines - 1):\n",
    "            rand1 = rn.randint(0, x)\n",
    "            rand2 = rn.randint(0, x - 1)\n",
    "\n",
    "            if rand1 == rand2:\n",
    "                rand2 += 1\n",
    "\n",
    "            if rand1 > rand2:\n",
    "                rand1, rand2 = rand2, rand1\n",
    "\n",
    "            if predictions_from_models[(cuisines_numbers[rand1], cuisines_numbers[rand2])][m] == cuisines_numbers[rand1]:\n",
    "                cuisines_numbers.remove(cuisines_numbers[rand2])\n",
    "            else:\n",
    "                cuisines_numbers.remove(cuisines_numbers[rand1])\n",
    "\n",
    "            x -= 1\n",
    "\n",
    "        result[m] = cuisines_numbers[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the solution is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models\n"
     ]
    }
   ],
   "source": [
    "# Could take some time (like I said above)\n",
    "\n",
    "recipes = train.ingredients\n",
    "\n",
    "vect = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "# make one-hot vectors from recipes\n",
    "vectors = vect.fit_transform(recipes).todense()\n",
    "\n",
    "ingredients_one_hot_vectors = pd.DataFrame(data=vectors, columns=sorted(vect.vocabulary_))\n",
    "\n",
    "X_train = ingredients_one_hot_vectors\n",
    "y_train = train['cuisine']\n",
    "\n",
    "# cuisines to integers\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder = encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=123)\n",
    "\n",
    "\n",
    "data = pd.concat([X_train, pd.Series(y_train).rename(\"cuisine_type\")], axis=1)\n",
    "data.reset_index()\n",
    "\n",
    "cuisines_data = {}\n",
    "\n",
    "# split data about particular cuisine into one bucket\n",
    "for i in range(number_of_cuisines):\n",
    "    cuisine_i = data.loc[data[\"cuisine_type\"] == i]\n",
    "    cuisines_data[i] = (cuisine_i.drop(columns=\"cuisine_type\"), cuisine_i[\"cuisine_type\"])\n",
    "\n",
    "# For submited solution I used n_estimators=500 but it requires around 12 GB RAM so here I cut it a little\n",
    "# it may take some time to build all models especially with 500\n",
    "et_pipeline_pair_models = Pipeline([\n",
    "    ('classifier', ExtraTreesClassifier(n_estimators=50, random_state=671232, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Building models\")\n",
    "\n",
    "for i in range(number_of_cuisines):\n",
    "    for j in range(i + 1, number_of_cuisines):\n",
    "        X = pd.concat([cuisines_data[i][0], cuisines_data[j][0]], axis=0)\n",
    "        y = pd.concat([cuisines_data[i][1], cuisines_data[j][1]], axis=0)\n",
    "        models[(i, j)] = et_pipeline_pair_models.fit(X, y)\n",
    "        et_pipeline_pair_models = clone(et_pipeline_pair_models)\n",
    "        \n",
    "# Used for testing with split (split also commented above)\n",
    "# predicting\n",
    "#prediction = predict(X_test)\n",
    "\n",
    "# check accuracy\n",
    "#print(accuracy_score(prediction, y_test))\n",
    "        \n",
    "test = pd.read_json('cooking_test.json')\n",
    "X_test = test['ingredients']\n",
    "\n",
    "vectors = vect.transform(X_test).todense()\n",
    "\n",
    "X_test = pd.DataFrame(data=vectors, columns=sorted(vect.vocabulary_))\n",
    "\n",
    "prediction = predict(X_test)\n",
    "\n",
    "prediction = encoder.inverse_transform(prediction.astype(int))\n",
    "\n",
    "# path to directory where predictions shall be placed\n",
    "result_name = \"jk394342_predictions_solution2.csv\"\n",
    "\n",
    "submission = test.copy()\n",
    "submission['cuisine'] = prediction\n",
    "submission.to_csv(result_name, index=False, columns=['id', 'cuisine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried some solutions with nerual networks but it gave me worse results so I stayed with this solutions. Maybe I did sth wrong. I was sure that neural networks should gave better result but well ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jk394342_assignment2",
   "language": "python",
   "name": "jk394342_assignment2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
